{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68e68e5-3787-42c8-ae4b-ebeb3b9367de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "a = torch.arange(1, 17)  # a's shape is (16,)\n",
    " \n",
    "a.view(-1,2,2) # output below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6f7420-7404-4e67-8531-7d280eb56592",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[32004, 32005,   474,  1303, 10081,  8277,   263,  1629, 29889, 32000,\n",
    "           474, 29915, 29885,   263,   380,  1657,  3765,   408,   590,  1473,\n",
    "          4982, 29889, 32000,   474,   871, 17545,   413,   359,  2276, 29889,\n",
    "         32000,   474,   471, 10425,   297,   263,  2323,  3847, 22329, 29889,\n",
    "         32000, 32002, 22172,   825,   526,  2599,  9826,  1577,     2]],\n",
    "       device='cuda:0')\n",
    "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1]], device='cuda:0')\n",
    "\n",
    "tensor([[50267, 50268,   939,   524, 34574,    11, 47510,  2292, 28627,     8,\n",
    "         41259,     4,     2,   127,  6578,     8,   939,    32,  1375,    88,\n",
    "            41,  3537,   561,   220,   186,     4,     2,   127,  2674,  8089,\n",
    "            32,  1275,     8,  4334,     4,     2,   939,   524,  2509,    11,\n",
    "         11075,     8,   101,   602,  3493,     4,     2,   939,   524,    41,\n",
    "         10891,   334,  3254,     4,     2, 50265,   141,    32,    47,   608,\n",
    "           452, 17487,     2]], device='cuda:0')\n",
    "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "215d430d-5474-4acc-bd7b-4ea69809dc6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers \n",
    "from transformers import AutoTokenizer\n",
    "from transformers import LlamaForCausalLM\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "def prepare_llama_tokenizer_and_embedding(\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        model: transformers.PreTrainedModel,\n",
    "        special_tokens_dict: Dict = dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "):\n",
    "    \"\"\"prepare llama tokenizer and embedding.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "    tokenizer.add_special_tokens({\n",
    "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "    })\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97054cdf-4e48-44b9-a96a-de0be0a65cee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_mine = pad_sequence([torch.from_numpy(np.array(x)) for x in item_mine],\n",
    "                                  batch_first=True, padding_value=pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5441c520-ed41-4fd0-bc02-0ccee6dcf73d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"/checkpoint/llama_7B/\", padding_side=\"right\", use_fast=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./checkpoint/llama_7B\",\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "#model = LlamaForCausalLM.from_pretrained(\"./checkpoint/llama_7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342c4ef-0678-473f-99a8-6add42e3ca76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.eos_token = '<\\s>'\n",
    "tokenizer.bos_token = '<s>'\n",
    "special_tokens_dict = {\"sep_token\": \"<\\sep>\", \"pad_token\": \"<\\pad>\"}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "tokenizer.add_tokens(['<query>', '<response>', '<latent>', '<persona>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160716f5-a72d-4b5e-8229-a42edaa2aea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id, tokenizer.sep_token_id)\n",
    "print(tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token, tokenizer.sep_token)\n",
    "print(tokenizer.convert_tokens_to_ids(['<query>', '<response>', '<latent>', '<persona>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab996e3-21a2-465c-bc88-1d0c56694670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76237327-dfb1-4581-a0b0-16b799c2f1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.sep_token='<\\s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8c748-f29b-404d-8946-3acac874057d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.eos_token = '<\\s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ca2868-ec20-4a1e-b990-e6b5825a2afb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b11c36-a555-46ae-a0b9-6ad1ac34a774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = '<\\s>'\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed0c49-198c-44b3-87a7-f614f5802eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "print(generate_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6fbaf7-3abb-4e92-9128-ceba6c68a3be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f8f46e-c7d7-4c94-9735-2208535aaf0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0a251c-7f0e-475e-ac87-ceedfda55725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(['hello']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7370622d-7aac-41cf-a943-a601a1080c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86786ed5-0d7f-42d0-b22d-ef94df35efa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd7c6b-0787-498c-926d-295339a7603b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = {v : k for k, v in a.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b15bd0-bdfa-46bf-8cca-d57ddb3da06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(a[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350a9aa-cff4-4ff6-8777-689d002f90c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2DoubleHeadsModel.from_pretrained('gpt2')\n",
    "\n",
    "choices = [ \"Bob likes candy ; what does Bob like ?  Bag <|endoftext|>\",\n",
    "                   \"Bob likes candy ; what does Bob like ?  Burger <|endoftext|>\",\n",
    "                   \"Bob likes candy ; what does Bob like ?  Candy <|endoftext|>\",\n",
    "                  \"Bob likes candy ; what does Bob like ?  Apple <|endoftext|>\"]\n",
    "\n",
    "encoded_choices = [tokenizer.encode(s) for s in choices]\n",
    "\n",
    "eos_token_location = [tokens.index(tokenizer.eos_token_id) for tokens in encoded_choices]\n",
    "input_ids = torch.tensor(encoded_choices).unsqueeze(0) \n",
    "mc_token_ids = torch.tensor([eos_token_location]) \n",
    "print(input_ids.shape)\n",
    "print(mc_token_ids.shape)\n",
    "outputs = model(input_ids, mc_token_ids=mc_token_ids)\n",
    "lm_prediction_scores, mc_prediction_scores = outputs[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33719677-a29c-4fd4-866d-02038c334065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(lm_prediction_scores.shape)\n",
    "print(mc_prediction_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dfcc11-a847-45dd-8eee-e7e8c59e2a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb, random\n",
    "total_step_num = 1000\n",
    "for step in range(total_step_num):\n",
    "    wandb.log({'random_curve':step/100+random.random()},step=step)\n",
    "    wandb.log({'log_curve': math.log(step+1)},step=step)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8a4f1-2750-472d-9f5a-f455d1486a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e048369-20c7-4126-85f2-fc8f9f9eb082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Launch 5 simulated experiments\n",
    "total_runs = 5\n",
    "for run in range(total_runs):\n",
    "  # 🐝 1️⃣ Start a new run to track this script\n",
    "  wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"wandbexample1\", \n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"experiment_{run}\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": 0.02,\n",
    "      \"architecture\": \"CNN\",\n",
    "      \"dataset\": \"CIFAR-100\",\n",
    "      \"epochs\": 10,\n",
    "      })\n",
    "  \n",
    "  # This simple block simulates a training loop logging metrics\n",
    "  epochs = 10\n",
    "  offset = random.random() / 5\n",
    "  for epoch in range(2, epochs):\n",
    "      acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "      loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "      \n",
    "      # 🐝 2️⃣ Log metrics from your script to W&B\n",
    "      wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "      \n",
    "  # Mark the run as finished\n",
    "  wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084745ef-0c3d-4f1b-bca3-34eaeb55f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "['model.layers.0.self_attn.o_proj.lora_B', \n",
    " 'model.layers.24.mlp.up_proj.lora_B', \n",
    " 'model.layers.12.mlp.down_proj.lora_B', \n",
    " 'model.layers.8.self_attn.v_proj.lora_A', \n",
    " 'model.layers.12.self_attn.q_proj.lora_B', \n",
    " 'model.layers.21.mlp.up_proj.lora_B', \n",
    " 'model.layers.20.mlp.gate_proj.lora_B', 'model.layers.17.mlp.gate_proj.lora_B', 'model.layers.7.mlp.down_proj.lora_A', 'model.layers.14.mlp.up_proj.lora_A', 'lm_head.lora_A', 'model.layers.28.self_attn.o_proj.lora_A', 'model.layers.19.self_attn.v_proj.lora_A', 'model.layers.8.mlp.up_proj.lora_B', 'model.layers.31.mlp.gate_proj.lora_A', 'model.layers.23.mlp.down_proj.lora_A', 'model.layers.17.self_attn.k_proj.lora_A', 'model.layers.19.self_attn.q_proj.lora_A', 'model.layers.0.mlp.up_proj.lora_B', 'model.layers.15.self_attn.v_proj.lora_B', 'model.layers.5.self_attn.q_proj.lora_B', 'model.layers.20.mlp.up_proj.lora_A', 'model.layers.26.mlp.up_proj.lora_B', 'model.layers.25.self_attn.v_proj.lora_B', 'model.layers.6.mlp.gate_proj.lora_B', 'model.layers.21.self_attn.q_proj.lora_A', 'model.layers.9.self_attn.v_proj.lora_A', 'model.layers.16.mlp.down_proj.lora_A', 'model.layers.17.self_attn.q_proj.lora_B', 'model.layers.0.self_attn.k_proj.lora_A', 'model.layers.4.mlp.down_proj.lora_A', 'model.layers.22.mlp.down_proj.lora_A', 'model.layers.24.mlp.up_proj.lora_A', 'model.layers.3.mlp.down_proj.lora_B', 'model.layers.25.mlp.up_proj.lora_B', 'model.layers.3.self_attn.k_proj.lora_B', 'model.layers.13.mlp.gate_proj.lora_B', 'model.layers.15.mlp.gate_proj.lora_A', 'model.layers.22.self_attn.o_proj.lora_B', 'model.layers.27.self_attn.q_proj.lora_A', 'model.layers.1.self_attn.k_proj.lora_B', 'model.layers.27.mlp.down_proj.lora_A', 'model.layers.30.self_attn.q_proj.lora_A', 'model.layers.20.self_attn.v_proj.lora_A', 'model.layers.13.self_attn.q_proj.lora_A', 'model.layers.30.self_attn.k_proj.lora_A', 'model.layers.9.self_attn.k_proj.lora_B', 'model.layers.17.self_attn.v_proj.lora_A', 'model.layers.17.self_attn.o_proj.lora_B', 'model.layers.24.self_attn.o_proj.lora_A', 'model.layers.31.mlp.gate_proj.lora_B', 'model.layers.1.self_attn.q_proj.lora_A', 'model.layers.24.mlp.down_proj.lora_A', 'model.layers.7.mlp.gate_proj.lora_B', 'model.layers.15.self_attn.k_proj.lora_B', 'model.layers.2.self_attn.k_proj.lora_A', 'model.layers.6.self_attn.k_proj.lora_A', 'model.layers.27.self_attn.o_proj.lora_B', 'model.layers.27.self_attn.v_proj.lora_B', 'model.layers.29.self_attn.k_proj.lora_B', 'model.layers.25.mlp.down_proj.lora_A', 'model.layers.2.mlp.down_proj.lora_A', 'model.layers.14.self_attn.v_proj.lora_B', 'model.layers.29.self_attn.v_proj.lora_A', 'model.layers.26.self_attn.o_proj.lora_A', 'model.layers.25.self_attn.o_proj.lora_B', 'model.layers.14.mlp.down_proj.lora_A', 'model.layers.20.self_attn.k_proj.lora_A', 'model.layers.27.self_attn.q_proj.lora_B', 'model.layers.31.mlp.up_proj.lora_A', 'model.layers.22.mlp.down_proj.lora_B', 'model.layers.7.self_attn.o_proj.lora_A', 'model.layers.8.self_attn.o_proj.lora_B', 'model.layers.2.self_attn.k_proj.lora_B', 'model.layers.15.self_attn.q_proj.lora_B', 'model.layers.23.self_attn.q_proj.lora_A', 'model.layers.3.mlp.up_proj.lora_B', 'model.layers.7.self_attn.k_proj.lora_B', 'model.layers.5.self_attn.v_proj.lora_B', 'model.layers.20.self_attn.q_proj.lora_B', 'model.layers.7.self_attn.v_proj.lora_B', 'model.layers.11.self_attn.o_proj.lora_B', 'model.layers.31.mlp.up_proj.lora_B', 'model.layers.15.self_attn.k_proj.lora_A', 'model.layers.22.mlp.gate_proj.lora_B', 'model.layers.16.self_attn.q_proj.lora_A', 'model.layers.4.mlp.gate_proj.lora_B', 'model.layers.11.self_attn.v_proj.lora_A', 'model.layers.19.mlp.gate_proj.lora_A', 'model.layers.3.self_attn.k_proj.lora_A', 'model.layers.0.mlp.gate_proj.lora_B', 'model.layers.0.mlp.down_proj.lora_B', 'model.layers.17.self_attn.k_proj.lora_B', 'model.layers.29.self_attn.k_proj.lora_A', 'model.layers.6.self_attn.k_proj.lora_B', 'model.layers.26.mlp.up_proj.lora_A', 'model.layers.12.self_attn.k_proj.lora_B', 'model.layers.7.self_attn.q_proj.lora_A', 'model.layers.4.mlp.down_proj.lora_B', 'model.layers.18.self_attn.o_proj.lora_B', 'model.layers.24.self_attn.o_proj.lora_B', 'model.layers.10.self_attn.o_proj.lora_A', 'model.layers.22.self_attn.q_proj.lora_B', 'model.layers.27.self_attn.v_proj.lora_A', 'model.layers.5.self_attn.v_proj.lora_A', 'model.layers.24.mlp.gate_proj.lora_B', 'model.layers.15.mlp.up_proj.lora_B', 'model.layers.26.self_attn.v_proj.lora_B', 'model.layers.23.mlp.up_proj.lora_B', 'model.layers.30.mlp.up_proj.lora_A', 'model.layers.4.self_attn.q_proj.lora_B', 'model.layers.8.mlp.gate_proj.lora_A', 'model.layers.21.mlp.gate_proj.lora_B', 'model.layers.22.self_attn.v_proj.lora_B', 'model.layers.23.self_attn.q_proj.lora_B', 'model.layers.11.mlp.gate_proj.lora_B', 'model.layers.25.mlp.gate_proj.lora_A', 'model.layers.6.self_attn.q_proj.lora_A', 'model.layers.6.mlp.up_proj.lora_A', 'model.layers.6.mlp.down_proj.lora_B', 'model.layers.8.self_attn.k_proj.lora_B', 'model.layers.21.self_attn.o_proj.lora_A', 'model.layers.23.self_attn.o_proj.lora_A', 'model.layers.9.mlp.down_proj.lora_A', 'model.layers.7.mlp.gate_proj.lora_A', 'model.layers.26.self_attn.k_proj.lora_A', 'model.layers.7.self_attn.q_proj.lora_B', 'model.layers.0.mlp.up_proj.lora_A', 'model.layers.28.self_attn.v_proj.lora_B', 'model.layers.18.mlp.down_proj.lora_A', 'model.layers.11.mlp.up_proj.lora_A', 'model.layers.12.self_attn.o_proj.lora_A', 'model.layers.2.mlp.up_proj.lora_B', 'model.layers.1.mlp.up_proj.lora_B', 'model.layers.14.self_attn.q_proj.lora_A', 'model.layers.30.mlp.down_proj.lora_A', 'model.layers.13.mlp.gate_proj.lora_A', 'model.layers.5.mlp.down_proj.lora_B', 'model.layers.26.mlp.down_proj.lora_B', 'model.layers.23.self_attn.v_proj.lora_B', 'model.layers.22.mlp.up_proj.lora_B', 'model.layers.24.self_attn.k_proj.lora_A', 'model.layers.16.self_attn.q_proj.lora_B', 'model.layers.2.self_attn.v_proj.lora_B', 'model.layers.28.mlp.up_proj.lora_A', 'model.layers.3.self_attn.q_proj.lora_B', 'model.layers.25.self_attn.k_proj.lora_B', 'model.layers.14.mlp.up_proj.lora_B', 'model.layers.25.mlp.up_proj.lora_A', 'model.layers.8.mlp.down_proj.lora_B', 'model.layers.16.self_attn.k_proj.lora_B', 'model.layers.23.self_attn.k_proj.lora_B', 'model.layers.31.self_attn.o_proj.lora_B', 'model.layers.15.mlp.gate_proj.lora_B', 'model.layers.1.mlp.gate_proj.lora_B', 'model.layers.23.self_attn.v_proj.lora_A', 'model.layers.12.mlp.gate_proj.lora_A', 'model.layers.28.mlp.gate_proj.lora_A', 'model.layers.9.mlp.down_proj.lora_B', 'model.layers.19.self_attn.q_proj.lora_B', 'model.layers.29.self_attn.q_proj.lora_A', 'model.layers.25.mlp.gate_proj.lora_B', 'model.layers.20.mlp.up_proj.lora_B', 'model.layers.3.self_attn.o_proj.lora_A', 'model.layers.29.mlp.gate_proj.lora_B', 'model.layers.26.self_attn.o_proj.lora_B', 'model.layers.3.self_attn.o_proj.lora_B', 'model.layers.6.mlp.down_proj.lora_A', 'model.layers.7.mlp.down_proj.lora_B', 'model.layers.28.mlp.down_proj.lora_A', 'model.layers.11.self_attn.k_proj.lora_A', 'model.layers.15.mlp.down_proj.lora_B', 'model.layers.23.mlp.gate_proj.lora_B', 'model.layers.2.self_attn.q_proj.lora_B', 'model.layers.4.mlp.up_proj.lora_A', 'model.layers.21.self_attn.k_proj.lora_A', 'model.layers.16.self_attn.v_proj.lora_B', 'model.layers.31.self_attn.q_proj.lora_B', 'model.layers.16.mlp.down_proj.lora_B', 'model.layers.13.mlp.up_proj.lora_B', 'model.layers.24.self_attn.v_proj.lora_A', 'model.layers.1.mlp.gate_proj.lora_A', 'model.layers.15.self_attn.v_proj.lora_A', 'model.layers.13.mlp.up_proj.lora_A', 'model.layers.16.mlp.up_proj.lora_B', 'model.layers.20.mlp.down_proj.lora_A', 'model.layers.27.mlp.down_proj.lora_B', 'model.layers.11.self_attn.v_proj.lora_B', 'model.layers.16.mlp.gate_proj.lora_A', 'model.layers.25.self_attn.q_proj.lora_B', 'model.layers.15.self_attn.o_proj.lora_B', 'model.layers.12.mlp.up_proj.lora_B', 'model.layers.21.self_attn.k_proj.lora_B', 'model.layers.8.mlp.up_proj.lora_A', 'model.layers.5.mlp.gate_proj.lora_A', 'model.layers.17.self_attn.o_proj.lora_A', 'model.layers.31.mlp.down_proj.lora_A', 'model.layers.25.self_attn.v_proj.lora_A', 'model.layers.15.mlp.down_proj.lora_A', 'model.layers.21.self_attn.v_proj.lora_A', 'model.layers.27.mlp.up_proj.lora_A', 'model.layers.14.self_attn.o_proj.lora_B', 'model.layers.10.self_attn.q_proj.lora_B', 'model.layers.14.self_attn.q_proj.lora_B', 'model.layers.13.self_attn.o_proj.lora_A', 'model.layers.7.self_attn.o_proj.lora_B', 'model.layers.11.mlp.up_proj.lora_B', 'model.layers.9.mlp.gate_proj.lora_B', 'model.layers.27.self_attn.k_proj.lora_B', 'model.layers.29.mlp.down_proj.lora_B', 'model.layers.13.self_attn.k_proj.lora_A', 'model.layers.30.mlp.down_proj.lora_B', 'model.layers.11.mlp.gate_proj.lora_A', 'model.layers.1.self_attn.o_proj.lora_A', 'model.layers.17.self_attn.q_proj.lora_A', 'model.layers.10.self_attn.v_proj.lora_A', 'model.layers.4.self_attn.q_proj.lora_A', 'model.layers.13.self_attn.q_proj.lora_B', 'model.layers.23.self_attn.o_proj.lora_B', 'model.layers.5.self_attn.o_proj.lora_B', 'model.layers.23.mlp.gate_proj.lora_A', 'model.layers.18.self_attn.k_proj.lora_A', 'model.layers.16.self_attn.k_proj.lora_A', 'model.layers.24.mlp.gate_proj.lora_A', 'model.layers.1.self_attn.o_proj.lora_B', 'model.layers.4.self_attn.v_proj.lora_B', 'model.layers.23.mlp.up_proj.lora_A', 'model.layers.30.mlp.gate_proj.lora_B', 'model.layers.1.mlp.down_proj.lora_B', 'model.layers.17.mlp.gate_proj.lora_A', 'model.layers.20.self_attn.q_proj.lora_A', 'model.layers.9.mlp.gate_proj.lora_A', 'model.layers.13.mlp.down_proj.lora_A', 'model.layers.2.self_attn.o_proj.lora_B', 'model.layers.21.self_attn.o_proj.lora_B', 'model.layers.4.self_attn.k_proj.lora_B', 'model.layers.25.mlp.down_proj.lora_B', 'model.layers.16.self_attn.o_proj.lora_A', 'model.layers.30.self_attn.q_proj.lora_B', 'model.layers.7.self_attn.v_proj.lora_A', 'model.layers.9.self_attn.q_proj.lora_B', 'model.layers.18.self_attn.v_proj.lora_B', 'model.layers.16.self_attn.v_proj.lora_A', 'model.layers.23.mlp.down_proj.lora_B', 'model.layers.11.self_attn.q_proj.lora_A', 'model.layers.1.mlp.up_proj.lora_A', 'model.layers.19.self_attn.o_proj.lora_B', 'model.layers.29.mlp.gate_proj.lora_A', 'model.layers.31.self_attn.k_proj.lora_B', 'model.layers.28.mlp.up_proj.lora_B', 'model.layers.25.self_attn.o_proj.lora_A', 'model.layers.7.mlp.up_proj.lora_B', 'model.layers.12.self_attn.v_proj.lora_B', 'model.layers.29.self_attn.q_proj.lora_B', 'model.layers.9.self_attn.v_proj.lora_B', 'model.layers.19.mlp.up_proj.lora_B', 'model.layers.10.mlp.up_proj.lora_A', 'model.layers.6.self_attn.v_proj.lora_A', 'model.layers.14.self_attn.o_proj.lora_A', 'model.layers.23.self_attn.k_proj.lora_A', 'model.layers.14.self_attn.v_proj.lora_A', 'model.layers.8.mlp.down_proj.lora_A', 'model.layers.2.mlp.gate_proj.lora_A', 'model.layers.13.mlp.down_proj.lora_B', 'model.layers.22.self_attn.q_proj.lora_A', 'model.layers.8.self_attn.q_proj.lora_A', 'model.layers.13.self_attn.o_proj.lora_B', 'model.layers.30.self_attn.o_proj.lora_A', 'model.layers.28.self_attn.q_proj.lora_A', 'model.layers.12.mlp.down_proj.lora_A', 'model.layers.9.self_attn.o_proj.lora_A', 'model.layers.31.self_attn.v_proj.lora_B', 'model.layers.31.self_attn.o_proj.lora_A', 'model.layers.1.self_attn.v_proj.lora_B', 'model.layers.20.mlp.gate_proj.lora_A', 'model.layers.6.self_attn.v_proj.lora_B', 'model.layers.17.mlp.down_proj.lora_A', 'model.layers.10.mlp.gate_proj.lora_A', 'model.layers.11.mlp.down_proj.lora_A', 'model.layers.3.self_attn.v_proj.lora_B', 'model.layers.11.self_attn.o_proj.lora_A', 'model.layers.20.mlp.down_proj.lora_B', 'model.layers.18.mlp.gate_proj.lora_B', 'model.layers.15.mlp.up_proj.lora_A', 'model.layers.31.self_attn.k_proj.lora_A', 'model.layers.10.self_attn.k_proj.lora_B', 'model.layers.30.mlp.gate_proj.lora_A', 'model.layers.25.self_attn.q_proj.lora_A', 'model.layers.3.self_attn.v_proj.lora_A', 'model.layers.18.mlp.up_proj.lora_B', 'model.layers.7.mlp.up_proj.lora_A', 'model.layers.19.self_attn.k_proj.lora_B', 'model.layers.20.self_attn.k_proj.lora_B', 'model.layers.8.mlp.gate_proj.lora_B', 'model.layers.19.mlp.down_proj.lora_B', 'model.layers.31.self_attn.v_proj.lora_A', 'model.layers.6.self_attn.o_proj.lora_A', 'model.layers.0.mlp.down_proj.lora_A', 'model.layers.19.mlp.up_proj.lora_A', 'model.layers.22.self_attn.v_proj.lora_A', 'model.layers.26.self_attn.q_proj.lora_B', 'model.layers.20.self_attn.o_proj.lora_B', 'model.layers.24.mlp.down_proj.lora_B', 'model.layers.25.self_attn.k_proj.lora_A', 'model.layers.26.self_attn.q_proj.lora_A', 'model.layers.21.mlp.down_proj.lora_A', 'model.layers.4.self_attn.o_proj.lora_B', 'model.layers.28.self_attn.k_proj.lora_B', 'model.layers.26.mlp.gate_proj.lora_A', 'model.layers.11.mlp.down_proj.lora_B', 'model.layers.10.self_attn.v_proj.lora_B', 'model.layers.3.self_attn.q_proj.lora_A', 'model.layers.2.self_attn.o_proj.lora_A', 'model.layers.10.self_attn.o_proj.lora_B', 'model.layers.26.mlp.down_proj.lora_A', 'model.layers.0.self_attn.v_proj.lora_B', 'model.layers.5.self_attn.k_proj.lora_B', 'model.layers.3.mlp.down_proj.lora_A', 'model.layers.22.self_attn.k_proj.lora_A', 'model.layers.29.mlp.down_proj.lora_A', 'model.layers.0.self_attn.q_proj.lora_A', 'model.layers.0.mlp.gate_proj.lora_A', 'model.layers.17.mlp.down_proj.lora_B', 'model.layers.2.self_attn.q_proj.lora_A', 'model.layers.9.mlp.up_proj.lora_B', 'model.layers.17.mlp.up_proj.lora_A', 'model.layers.2.mlp.up_proj.lora_A', 'model.layers.19.self_attn.v_proj.lora_B', 'model.layers.31.self_attn.q_proj.lora_A', 'model.layers.2.mlp.down_proj.lora_B', 'model.layers.15.self_attn.o_proj.lora_A', 'model.layers.1.mlp.down_proj.lora_A', 'model.layers.2.self_attn.v_proj.lora_A', 'model.layers.12.self_attn.o_proj.lora_B', 'model.layers.29.self_attn.o_proj.lora_A', 'model.layers.12.mlp.up_proj.lora_A', 'model.layers.19.mlp.gate_proj.lora_B', 'model.layers.8.self_attn.k_proj.lora_A', 'model.layers.27.mlp.gate_proj.lora_A', 'model.layers.29.mlp.up_proj.lora_B', 'model.layers.0.self_attn.q_proj.lora_B', 'model.layers.11.self_attn.q_proj.lora_B', 'model.layers.12.self_attn.k_proj.lora_A', 'model.layers.12.self_attn.v_proj.lora_A', 'model.layers.6.self_attn.q_proj.lora_B', 'model.layers.21.self_attn.v_proj.lora_B', 'model.layers.1.self_attn.k_proj.lora_A', 'model.layers.15.self_attn.q_proj.lora_A', 'model.layers.3.mlp.gate_proj.lora_A', 'model.layers.0.self_attn.o_proj.lora_A', 'model.layers.12.self_attn.q_proj.lora_A', 'model.layers.18.mlp.down_proj.lora_B', 'model.layers.16.mlp.up_proj.lora_A', 'model.layers.16.self_attn.o_proj.lora_B', 'model.layers.8.self_attn.o_proj.lora_A', 'model.layers.19.self_attn.o_proj.lora_A', 'model.layers.26.self_attn.k_proj.lora_B', 'model.layers.14.mlp.down_proj.lora_B', 'model.layers.31.mlp.down_proj.lora_B', 'model.layers.4.self_attn.o_proj.lora_A', 'model.layers.8.self_attn.q_proj.lora_B', 'model.layers.4.mlp.gate_proj.lora_A', 'model.layers.5.mlp.down_proj.lora_A', 'model.layers.9.self_attn.k_proj.lora_A', 'model.layers.5.mlp.up_proj.lora_B', 'model.layers.30.self_attn.o_proj.lora_B', 'model.layers.6.mlp.up_proj.lora_B', 'model.layers.24.self_attn.q_proj.lora_B', 'model.layers.28.mlp.gate_proj.lora_B', 'model.layers.8.self_attn.v_proj.lora_B', 'model.layers.22.mlp.up_proj.lora_A', 'model.layers.22.self_attn.o_proj.lora_A', 'model.layers.5.self_attn.k_proj.lora_A', 'model.layers.5.mlp.up_proj.lora_A', 'model.layers.24.self_attn.q_proj.lora_A', 'model.layers.28.mlp.down_proj.lora_B', 'model.layers.3.mlp.gate_proj.lora_B', 'model.layers.3.mlp.up_proj.lora_A', 'model.layers.2.mlp.gate_proj.lora_B', 'model.layers.18.self_attn.o_proj.lora_A', 'model.layers.28.self_attn.o_proj.lora_B', 'model.layers.18.mlp.up_proj.lora_A', 'model.layers.16.mlp.gate_proj.lora_B', 'model.layers.4.mlp.up_proj.lora_B', 'model.layers.18.self_attn.k_proj.lora_B', 'model.layers.28.self_attn.q_proj.lora_B', 'model.layers.5.self_attn.q_proj.lora_A', 'model.layers.10.mlp.down_proj.lora_B', 'model.layers.21.mlp.down_proj.lora_B', 'model.layers.24.self_attn.k_proj.lora_B', 'model.layers.27.self_attn.k_proj.lora_A', 'model.layers.13.self_attn.k_proj.lora_B', 'model.layers.10.mlp.down_proj.lora_A', 'model.layers.21.mlp.gate_proj.lora_A', 'model.layers.10.mlp.up_proj.lora_B', 'model.layers.14.self_attn.k_proj.lora_B', 'model.layers.27.mlp.gate_proj.lora_B', 'model.layers.6.self_attn.o_proj.lora_B', 'model.layers.1.self_attn.q_proj.lora_B', 'model.layers.29.self_attn.o_proj.lora_B', 'model.layers.29.mlp.up_proj.lora_A', 'model.layers.30.self_attn.k_proj.lora_B', 'model.layers.5.mlp.gate_proj.lora_B', 'lm_head.lora_B', 'model.layers.0.self_attn.k_proj.lora_B', 'model.layers.24.self_attn.v_proj.lora_B', 'model.layers.13.self_attn.v_proj.lora_B', 'model.layers.18.self_attn.q_proj.lora_B', 'model.layers.12.mlp.gate_proj.lora_B', 'model.layers.6.mlp.gate_proj.lora_A', 'model.layers.9.self_attn.q_proj.lora_A', 'model.layers.5.self_attn.o_proj.lora_A', 'model.layers.1.self_attn.v_proj.lora_A', 'model.layers.27.mlp.up_proj.lora_B', 'model.layers.11.self_attn.k_proj.lora_B', 'model.layers.28.self_attn.v_proj.lora_A', 'model.layers.30.self_attn.v_proj.lora_A', 'model.layers.4.self_attn.k_proj.lora_A', 'model.layers.0.self_attn.v_proj.lora_A', 'model.layers.18.self_attn.v_proj.lora_A', 'model.layers.18.self_attn.q_proj.lora_A', 'model.layers.30.self_attn.v_proj.lora_B', 'model.layers.30.mlp.up_proj.lora_B', 'model.layers.9.self_attn.o_proj.lora_B', 'model.layers.20.self_attn.v_proj.lora_B', 'model.layers.7.self_attn.k_proj.lora_A', 'model.layers.19.mlp.down_proj.lora_A', 'model.layers.18.mlp.gate_proj.lora_A', 'model.layers.26.mlp.gate_proj.lora_B', 'model.layers.17.self_attn.v_proj.lora_B', 'model.layers.4.self_attn.v_proj.lora_A', 'model.layers.28.self_attn.k_proj.lora_A', 'model.layers.14.mlp.gate_proj.lora_B', 'model.layers.21.mlp.up_proj.lora_A', 'model.layers.14.self_attn.k_proj.lora_A', 'model.layers.29.self_attn.v_proj.lora_B', 'model.layers.10.mlp.gate_proj.lora_B', 'model.layers.20.self_attn.o_proj.lora_A', 'model.layers.10.self_attn.q_proj.lora_A', 'model.layers.17.mlp.up_proj.lora_B', 'model.layers.26.self_attn.v_proj.lora_A', 'model.layers.13.self_attn.v_proj.lora_A', 'model.layers.27.self_attn.o_proj.lora_A', 'model.layers.22.self_attn.k_proj.lora_B', 'model.layers.21.self_attn.q_proj.lora_B', 'model.layers.9.mlp.up_proj.lora_A', 'model.layers.14.mlp.gate_proj.lora_A', 'model.layers.22.mlp.gate_proj.lora_A', 'model.layers.19.self_attn.k_proj.lora_A', 'model.layers.10.self_attn.k_proj.lora_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6354a-4dd8-433d-a674-aff21cbab151",
   "metadata": {},
   "outputs": [],
   "source": [
    " yes , i love fresh veggies . what do you do for a living ?\n",
    "--------------------------------------------------\n",
    " i don't have any pets , but i do travel a lot . i go to europe twice a year .\n",
    "--------------------------------------------------\n",
    " i travel to europe twice a year . do you have any family ?\n",
    "--------------------------------------------------\n",
    " my weekends are great , i go to europe twice a year .\n",
    "--------------------------------------------------\n",
    " i try to go at least once a year . do you have family ?\n",
    "--------------------------------------------------\n",
    " i've been to australia twice , it is beautiful . do you have family there ?\n",
    "--------------------------------------------------\n",
    " i do , i love to walk with my family . i am a descendant of christopher columbus .\n",
    "--------------------------------------------------\n",
    "274s elapsed: {'exs': 2194, '%done': '28.12%', 'time_left': '702s', 'f1': 0.2204, 'bleu': 0.01586}\n",
    " i am doing well . how are you ?\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58047499-fdc2-403f-867a-cd011a359bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------\n",
    "4738s elapsed: {'exs': 397, '%done': '5.09%', 'time_left': '88367s', 'f1': 0.03384, 'bleu': 0.0006517}\n",
    "<latent><persona> two dogs live with me. i've short hair. i like doing the treadmill and rowing machine. i go to the gym regularly.<query> hello , my name is leon . i am a audio engineer . how are you ?<response> hello , leon . my dogs and i are doing well .<query> that is good to hear ! i've been better , my girlfriend just left me .<response> oh no . i go to the gym a lot . the treadmill keeps my mind off things .<query> i tend to stick indoors but maybe the gym will help , thanks .<response> you are most welcome . the rowing machine is nice too .<query> that's a full body work out right there . do you live in california ? The 2018-19 school year is off to a great start! We are excited to welcome our new students and families to the school and to welcome back our returning students and families.\n",
    "We are looking forward to a great year of learning and growing together.\n",
    "Please take a moment to review the information below and the links to the left.\n",
    "If you have any questions, please contact the school office at 780-467-2200.\n",
    "The school office is open from 8:00 am to 4:00 pm.\n",
    "The school office is closed for lunch from 12:00 pm to 12:30 pm.\n",
    "The school office is closed for lunch from 12:00 pm to 12:30 pm.\n",
    "The school office is closed for lunch from 12:00 pm to 12:30 pm.\n",
    "The school office is closed for lunch from 12:00 pm to 12:30 pm.\n",
    "The school office is closed for lunch from 12:00 pm to 12:30 pm.\n",
    "The school office is closed for lunch from 12:00 pm to 12:30 pm.\n",
    "The school office is closed for lunch from 12:00 pm to 12:30 pm.\n",
    "The school office is closed for lunch from 12:00 pm to 12:30 pm.\n",
    "The school office is closed for lunch\n",
    "--------------------------------------------------\n",
    "4749s elapsed: {'exs': 398, '%done': '5.10%', 'time_left': '88351s', 'f1': 0.03387, 'bleu': 0.0006797}\n",
    "<latent><persona> two dogs live with me. i've short hair. i like doing the treadmill and rowing machine. i go to the gym regularly.<query> hello , my name is leon . i am a audio engineer . how are you ?<response> hello , leon . my dogs and i are doing well .<query> that is good to hear ! i've been better , my girlfriend just left me .<response> oh no . i go to the gym a lot . the treadmill keeps my mind off things .<query> i tend to stick indoors but maybe the gym will help , thanks .<response> you are most welcome . the rowing machine is nice too .<query> that's a full body work out right there . do you live in california ?<response> sadly , no . my dogs and i are in ohio .<query> i think i need to move . my studio is tanking after the decline of the market . The 2018-19 school year is off to a great start! We are excited to welcome our new students and families to the school and to welcome back our returning students and families.\n",
    "We are looking forward to a great year of learning and growing together.\n",
    "Please take a moment to review the information below and the links to the left.\n",
    "The first day of school is Tuesday, September 4th.\n",
    "The first day of school for students is Tuesday, September 4th.\n",
    "The first day of school for students is Wednesday, September 5th.\n",
    "The first day of school for students is Thursday, September 6th.\n",
    "The first day of school for students is Friday, September 7th.\n",
    "The first day of school for students is Monday, September 10th.\n",
    "The first day of school for students is Tuesday, September 111th.\n",
    "The first day of school for students is Wednesday, September 12th.\n",
    "The first day of school for students is Thursday, September 13th.\n",
    "The first day of school for students is Friday, September 14th.\n",
    "The first day of school for students is Monday, September 17th.\n",
    "The first day of school for students is Tuesday, September 18th.\n",
    "The first day of school for students is Wednesday,\n",
    "--------------------------------------------------\n",
    "4760s elapsed: {'exs': 399, '%done': '5.11%', 'time_left': '88312s', 'f1': 0.03386, 'bleu': 0.000678}\n",
    "<latent><persona> two dogs live with me. i've short hair. i like doing the treadmill and rowing machine. i go to the gym regularly.<query> hello , my name is leon . i am a audio engineer . how are you ?<response> hello , leon . my dogs and i are doing well .<query> that is good to hear ! i've been better , my girlfriend just left me .<response> oh no . i go to the gym a lot . the treadmill keeps my mind off things .<query> i tend to stick indoors but maybe the gym will help , thanks .<response> you are most welcome . the rowing machine is nice too .<query> that's a full body work out right there . do you live in california ?<response> sadly , no . my dogs and i are in ohio .<query> i think i need to move . my studio is tanking after the decline of the market .<response> oh no . i'm moving soon as well . due to personal reasons .<query> well i hope your reasons work themselves out . what kind of dogs do you have ? The 2018-19 school year is off to a great start! We are excited to welcome our new students and families to the school and to welcome back our returning students and families.\n",
    "We are looking forward to a great year of learning and growing together.\n",
    "Please take a moment to review the information below and the links to the left.\n",
    "The first day of school is Tuesday, September 4th.\n",
    "The first day of school for students is Tuesday, September 4th.\n",
    "The first day of school for students is Tuesday, September 4th. The first day of school for teachers is Monday, September 3rd.\n",
    "The first day of school for students is Tuesday, September 4th. The first day of school for teachers is Monday, September 3rd.\n",
    "The first day of school for students is Tuesday, September 4th. The first day of school for teachers is Monday, September 3rd. The first day of school for teachers is Monday, September 3rd.\n",
    "The first day of school for students is Tuesday, September 4th. The first day of school for teachers is Monday, September 3rd. The first day of school for teachers is Monday\n",
    "--------------------------------------------------\n",
    "4769s elapsed: {'exs': 400, '%done': '5.13%', 'time_left': '88251s', 'f1': 0.03386, 'bleu': 0.0006763}\n",
    "<latent><persona> two dogs live with me. i've short hair. i like doing the treadmill and rowing machine. i go to the gym regularly.<query> hello , my name is leon . i am a audio engineer . how are you ?<response> hello , leon . my dogs and i are doing well .<query> that is good to hear ! i've been better , my girlfriend just left me .<response> oh no . i go to the gym a lot . the treadmill keeps my mind off things .<query> i tend to stick indoors but maybe the gym will help , thanks .<response> you are most welcome . the rowing machine is nice too .<query> that's a full body work out right there . do you live in california ?<response> sadly , no . my dogs and i are in ohio .<query> i think i need to move . my studio is tanking after the decline of the market .<response> oh no . i'm moving soon as well . due to personal reasons .<query> well i hope your reasons work themselves out . what kind of dogs do you have ?<response> i've a springer spaniel and a lab . do you have pets ?<query> sadly no , they can be very destructive in an audio booth . The 2018-19 school year is off to a great start! We are excited to welcome our new students and families to the school and to welcome back our returning students and families.\n",
    "We are looking forward to a great year of learning and growing together.\n",
    "Please take a moment to review the information below and the links to the left.\n",
    "The first day of school is Tuesday, September 4th.\n",
    "The first day of school for students is Tuesday, September 4th.\n",
    "The first day of school for students is Tuesday, September 4th. The first day of school for students is Tuesday, September 4th.\n",
    "The first day of school for students is Tuesday, September 4th. The first day of school for students is Tuesday, September 4th.\n",
    "The first day of school for students is Tuesday, September 4th. The first day of school for students is Tuesday, September 4th. The first day of school for students is Tuesday, September\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6591f25-f5fb-4bf3-b05d-5896addaaa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'input_ids': tensor([[  366,  1818,   367,  1407,  5172,   869, 29826,   338,   697,   310,\n",
    "           590, 25448,   298, 20838,   583,   869,     2,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0],\n",
    "        [  474,   884,  1083, 27224, 17774,   746,   474,   626,   451,   714,\n",
    "         12580, 29826,   869,     2,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0]]), 'labels': tensor([[  366,  1818,   367,  1407,  5172,   869, 29826,   338,   697,   310,\n",
    "           590, 25448,   298, 20838,   583,   869,     2,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0],\n",
    "        [  474,   884,  1083, 27224, 17774,   746,   474,   626,   451,   714,\n",
    "         12580, 29826,   869,     2,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915c5f7-086c-4078-9001-a88a42f3319f",
   "metadata": {},
   "outputs": [],
   "source": [
    " 1577, 2, 32003, 4874, 1919, 474, 626, 515, 278, 1629, 29871, 29896, 29947, 29953, 29900, 869, 474, 471, 6296, 1244, 408, 263, 19532, 869, 2, 32002, 8031, 1738, 474, 29915, 29885, 6142, 263, 16336, 14497, 264, 322, 515, 393, 3152, 869, 738, 413, 4841, 1577, 2], [32004, 32005, 474, 471, 278, 937, 6345, 2278, 29889, 32000, 474, 19824, 322, 2678, 3897, 263, 1559, 29886, 5893, 29889, 32000, 474, 2355, 8300, 29871, 29896, 29900, 2440, 2678, 29889, 32000, 474, 471, 263, 19532, 363, 29871, 29896, 29900, 2440, 29889, 32000, 32002, 22172, 920, 526, 366, 445, 11005, 1577, 2, 32003, 22172, 1919, 474, 626, 2691, 6452, 366, 869, 474, 626, 925, 28321, 590, 521, 2361, 869, 2, 32002, 474, 626, 408, 1532, 869, 508, 366, 2649, 592, 263, 2586, 1048, 7535, 1577, 2, 32003, 4874, 1919, 474, 626, 515, 278, 1629, 29871, 29896, 29947, 29953, 29900, 869, 474, 471, 6296, 1244, 408, 263, 19532, 869, 2, 32002, 8031, 1738, 474, 29915, 29885, 6142, 263, 16336, 14497, 264, 322, 515, 393, 3152, 869, 738, 413, 4841, 1577, 2, 32003, 474, 29915, 29885, 8300, 304, 263, 10404, 19532, 322, 591, 750, 1023, 4344, 4208, 869, 2, 32002, 590, 10216, 471, 263, 6413, 899, 1241, 322, 591, 505, 29871, 29946, 413, 4841, 869, 2], [32004, 32005, 474, 471, 278, 937, 6345, 2278, 29889, 32000, 474, 19824, 322, 2678, 3897, 263, 1559, 29886, 5893, 29889, 32000, 474, 2355, 8300, 29871, 29896, 29900, 2440, 2678, 29889, 32000, 474, 471, 263, 19532, 363, 29871, 29896, 29900, 2440, 29889, 32000, 32002, 22172, 920, 526, 366, 445, 11005, 1577, 2, 32003, 22172, 1919, 474, 626, 2691, 6452, 366, 869, 474, 626, 925, 28321, 590, 521, 2361, 869, 2, 32002, 474, 626, 408, 1532, 869, 508, 366, 2649, 592, 263, 2586, 1048, 7535, 1577, 2, 32003, 4874, 1919, 474, 626, 515, 278, 1629, 29871, 29896, 29947, 29953, 29900, 869, 474, 471, 6296, 1244, 408, 263, 19532, 869, 2, 32002, 8031, 1738, 474, 29915, 29885, 6142, 263, 16336, 14497, 264, 322, 515, 393, 3152, 869, 738, 413, 4841, 1577, 2, 32003, 474, 29915, 29885, 8300, 304, 263, 10404, 19532, 322, 591, 750, 1023, 4344, 4208, 869, 2, 32002, 590, 10216, 471, 263, 6413, 899, 1241, 322, 591, 505, 29871, 29946, 413, 4841, 869, 2, 32003, 20695, 1738, 474, 881, 15544, 869, 590, 6532, 322, 474, 1716, 19824, 1156, 29871, 29896, 29900, 2440, 408, 25569, 869, 2, 32002, 393, 29915, 29879, 29811, 14981, 766, 354, 8109, 292, 304, 8293, 869, 2], [32004, 32005, 474, 471, 278, 937, 6345, 2278, 29889, 32000, 474, 19824, 322, 2678, 3897, 263, 1559, 29886, 5893, 29889, 32000, 474, 2355, 8300, 29871, 29896, 29900, 2440, 2678, 29889, 32000, 474, 471, 263, 19532, 363, 29871, 29896, 29900, 2440, 29889, 32000, 32002, 22172, 920, 526, 366, 445, 11005, 1577, 2, 32003, 22172, 1919, 474, 626, 2691, 6452, 366, 869, 474, 626, 925, 28321, 590, 521, 2361, 869, 2, 32002, 474, 626, 408, 1532, 869, 508, 366, 2649, 592, 263, 2586, 1048, 7535, 1577, 2, 32003, 4874, 1919, 474, 626, 515, 278, 1629, 29871, 29896, 29947, 29953, 29900, 869, 474, 471, 6296, 1244, 408, 263, 19532, 869, 2, 32002, 8031, 1738, 474, 29915, 29885, 6142, 263, 16336, 14497, 264, 322, 515, 393, 3152, 869, 738, 413, 4841, 1577, 2, 32003, 474, 29915, 29885, 8300, 304, 263, 10404, 19532, 322, 591, 750, 1023, 4344, 4208, 869, 2, 32002, 590, 10216, 471, 263, 6413, 899, 1241, 322, 591, 505, 29871, 29946, 413, 4841, 869, 2, 32003, 20695, 1738, 474, 881, 15544, 869, 590, 6532, 322, 474, 1716, 19824, 1156, 29871, 29896, 29900, 2440, 408, 25569, 869, 2, 32002, 393, 29915, 29879, 29811, 14981, 766, 354, 8109, 292, 304, 8293, 869, 2, 32003, 1532, 591, 19824, 322, 1539, 1449, 29871, 29896, 29900, 2440, 2678, 322, 892, 8300, 869, 591, 526, 9796, 869, 2, 32002, 393, 29915, 29879, 1781, 1738, 738, 4595, 29895, 4841, 3447, 1577, 2], [32004, 32005, 474, 471, 278, 937, 6345, 2278, 29889, 32000, 474, 19824, 322, 2678, 3897, 263, 1559, 29886, 5893, 29889, 32000, 474, 2355, 8300, 29871, 29896, 29900, 2440, 2678, 29889, 32000, 474, 471, 263, 19532, 363, 29871, 29896, 29900, 2440, 29889, 32000, 32002, 22172, 920, 526, 366, 445, 11005, 1577, 2, 32003, 22172, 1919, 474, 626, 2691, 6452, 366, 869, 474, 626, 925, 28321, 590, 521, 2361, 869, 2, 32002, 474, 626, 408, 1532, 869, 508, 366, 2649, 592, 263, 2586, 1048, 7535, 1577, 2, 32003, 4874, 1919, 474, 626, 515, 278, 1629, 29871, 29896, 29947, 29953, 29900, 869, 474, 471, 6296, 1244, 408, 263, 19532, 869, 2, 32002, 8031, 1738, 474, 29915, 29885, 6142, 263, 16336, 14497, 264, 322, 515, 393, 3152, 869, 738, 413, 4841, 1577, 2, 32003, 474, 29915, 29885, 8300, 304, 263, 10404, 19532, 322, 591, 750, 1023, 4344, 4208, 869, 2, 32002, 590, 10216, 471, 263, 6413, 899, 1241, 322, 591, 505, 29871, 29946, 413, 4841, 869, 2, 32003, 20695, 1738, 474, 881, 15544, 869, 590, 6532, 322, 474, 1716, 19824, 1156, 29871, 29896, 29900, 2440, 408, 25569, 869, 2, 32002, 393, 29915, 29879, 29811, 14981, 766, 354, 8109, 292, 304, 8293, 869, 2, 32003, 1532, 591, 19824, 322, 1539, 1449, 29871, 29896, 29900, 2440, 2678, 322, 892, 8300, 869, 591, 526, 9796, 869, 2, 32002, 393, 29915, 29879, 1781, 1738, 738, 4595, 29895, 4841, 3447, 1577, 2, 32003, 1716, 310, 1749, 4344, 750, 289, 370, 583, 869, 591, 505, 29871, 29945, 4595, 29881, 6334, 2153, 322, 1023, 22114, 787, 869, 2, 32002, 590, 10216, 322, 474, 29915, 345, 4595, 29895, 4841, 393, 3464, 515, 29871, 29955, 304, 29871, 29941, 29900, 869, 727, 526, 29871, 29896, 29900, 3001, 869, 2], [32004, 32005, 474, 471, 278, 937, 6345, 2278, 29889, 32000, 474, 19824, 322, 2678, 3897, 263, 1559, 29886, 5893, 29889, 32000, 474, 2355, 8300, 29871, 29896, 29900, 2440, 2678, 29889, 32000, 474, 471, 263, 19532, 363, 29871, 29896, 29900, 2440, 29889, 32000, 32002, 22172, 920, 526, 366, 445, 11005, 1577, 2, 32003, 22172, 1919, 474, 626, 2691, 6452, 366, 869, 474, 626, 925, 28321, 590, 521, 2361, 869, 2, 32002, 474, 626, 408, 1532, 869, 508, 366, 2649, 592, 263, 2586, 1048, 7535, 1577, 2, 32003, 4874, 1919, 474, 626, 515, 278, 1629, 29871, 29896, 29947, 29953, 29900, 869, 474, 471, 6296, 1244, 408, 263, 19532, 869, 2, 32002, 8031, 1738, 474, 29915, 29885, 6142, 263, 16336, 14497, 264, 322, 515, 393, 3152, 869, 738, 413, 4841, 1577, 2, 32003, 474, 29915, 29885, 8300, 304, 263, 10404, 19532, 322, 591, 750, 1023, 4344, 4208, 869, 2, 32002, 590, 10216, 471, 263, 6413, 899, 1241, 322, 591, 505, 29871, 29946, 413, 4841, 869, 2, 32003, 20695, 1738, 474, 881, 15544, 869, 590, 6532, 322, 474, 1716, 19824, 1156, 29871, 29896, 29900, 2440, 408, 25569, 869, 2, 32002, 393, 29915, 29879, 29811, 14981, 766, 354, 8109, 292, 304, 8293, 869, 2, 32003, 1532, 591, 19824, 322, 1539, 1449, 29871, 29896, 29900, 2440, 2678, 322, 892, 8300, 869, 591, 526, 9796, 869, 2, 32002, 393, 29915, 29879, 1781, 1738, 738, 4595, 29895, 4841, 3447, 1577, 2, 32003, 1716, 310, 1749, 4344, 750, 289, 370, 583, 869, 591, 505, 29871, 29945, 4595, 29881, 6334, 2153, 322, 1023, 22114, 787, 869, 2, 32002, 590, 10216, 322, 474, 29915, 345, 4595, 29895, 4841, 393, 3464, 515, 29871, 29955, 304, 29871, 29941, 29900, 869, 727, 526, 29871, 29896, 29900, 3001, 869, 2, 32003, 4595, 11991, 526, 263, 17065, 292, 1738, 474, 1010, 366, 13389, 963, 1738, 2, 32002, 896, 1854, 526, 869, 591, 437, 1738, 727, 526, 29871, 29947, 14000, 322, 29871, 29906, 12544, 869, 2]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
